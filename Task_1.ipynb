{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8620e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc784b4c",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bc97da",
   "metadata": {},
   "source": [
    "### 1.1 import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93fd3341",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import re\n",
    "import json\n",
    "\n",
    "import os\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from typing import List, TypedDict\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv() \n",
    "\n",
    "# Access the key\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e5a1af",
   "metadata": {},
   "source": [
    "### 1.2 extracting data from two pdf file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1e5e495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf_reading function\n",
    "def pdf_reading(pdf_path):\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        full_text = \"\"\n",
    "        for page in pdf.pages:\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                full_text += page_text + \"\\n\"\n",
    "        return full_text\n",
    "\n",
    "# Extract data from participant statement\n",
    "def extract_participant_statement_data(full_text):\n",
    "\n",
    "    # Participant statement data \n",
    "    participant_statement_data = {\n",
    "        \"account_holder\": None,\n",
    "        \"statement_date_range\": None,\n",
    "        \"total_portfolio_balance\": None,\n",
    "        \"asset_allocation\": {},\n",
    "        \"quarterly_contributions\": {\n",
    "            \"employee\": None,\n",
    "            \"employer\": None,\n",
    "            \"gains_loss\": None\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # 1. Account holder name\n",
    "    account_match = re.search(r\"Account holder:\\s*(.+)\", full_text)\n",
    "    if account_match:\n",
    "        name = account_match.group(1).strip()\n",
    "        if name and \"SAMPLE\" not in name:\n",
    "            participant_statement_data[\"account_holder\"] = name\n",
    "\n",
    "    # 2. Statement date range\n",
    "    date_match = re.search(r\"FOR\\s+([A-Za-z]+\\s+\\d{1,2},\\s*\\d{4})\\s+TO\\s+([A-Za-z]+\\s+\\d{1,2},\\s*\\d{4})\", full_text, re.IGNORECASE)\n",
    "    if date_match:\n",
    "        participant_statement_data[\"statement_date_range\"] = f\"{date_match.group(1)} to {date_match.group(2)}\"\n",
    "\n",
    "    # 3. Total portfolio balance as of statement end date\n",
    "    balance_match = re.search(r\"Ending balance\\s*\\$([\\d,]+\\.\\d{2})\", full_text)\n",
    "    if balance_match:\n",
    "        participant_statement_data[\"total_portfolio_balance\"] = balance_match.group(1)\n",
    "\n",
    "    # 4. Asset allocation breakdown\n",
    "    asset_section = re.search(r\"How your portfolio is allocated(.+?)These asset allocation\", full_text, re.DOTALL)\n",
    "    if asset_section:\n",
    "        for asset_type in [\"Equities\", \"Fixed Income\", \"Multi-Asset\"]:\n",
    "            match = re.search(rf\"{asset_type}\\s*\\$([\\d,]+\\.\\d{{2}})\\s*([\\d\\.]+)%\", asset_section.group(1))\n",
    "            if match:\n",
    "                participant_statement_data[\"asset_allocation\"][asset_type] = {\n",
    "                    \"value\": match.group(1),\n",
    "                    \"percent\": match.group(2)\n",
    "                }\n",
    "\n",
    "    # 5. Contributions and gains/loss for the quarter\n",
    "    contrib_match = re.search(\n",
    "        r\"Your contributions\\s*([\\d,]+\\.\\d{2}).*?Employer contributions\\s*([\\d,]+\\.\\d{2}).*?Gains/Loss\\s*([\\d,]+\\.\\d{2})\",\n",
    "        full_text, re.DOTALL\n",
    "    )\n",
    "    if contrib_match:\n",
    "        participant_statement_data[\"quarterly_contributions\"][\"employee\"] = contrib_match.group(1)\n",
    "        participant_statement_data[\"quarterly_contributions\"][\"employer\"] = contrib_match.group(2)\n",
    "        participant_statement_data[\"quarterly_contributions\"][\"gains_loss\"] = contrib_match.group(3)\n",
    "    return participant_statement_data\n",
    "\n",
    "\n",
    "# Extract data from retirement benefits\n",
    "def extract_retirement_benefit_data(full_text):\n",
    "\n",
    "    # Retirement benefits data\n",
    "    retirement_benefit_data = {\n",
    "        \"defined_benefit_vs_contribution\": None,\n",
    "        \"vesting_rules\": None,\n",
    "        \"participant_rights\": None\n",
    "    }\n",
    "\n",
    "    # 1. Defined benefit vs defined contribution\n",
    "    match1 = re.search(r\"(defined benefit.*?defined contribution.*?)(?:\\n\\n|\\Z)\", full_text, re.IGNORECASE | re.DOTALL)\n",
    "    if match1:\n",
    "        retirement_benefit_data[\"defined_benefit_vs_contribution\"] = match1.group(1).strip()\n",
    "\n",
    "    # 2. Vesting rules summary (cliff & graduated)\n",
    "    match2 = re.search(r\"(vesting.*?cliff.*?graduated.*?)(?:\\n\\n|\\Z)\", full_text, re.IGNORECASE | re.DOTALL)\n",
    "    if match2:\n",
    "        retirement_benefit_data[\"vesting_rules\"] = match2.group(1).strip()\n",
    "\n",
    "    # 3. Participant rights related to account balances\n",
    "    match3 = re.search(r\"(participant rights.*?account balance.*?)(?:\\n\\n|\\Z)\", full_text, re.IGNORECASE | re.DOTALL)\n",
    "    if match3:\n",
    "        retirement_benefit_data[\"participant_rights\"] = match3.group(1).strip()\n",
    "\n",
    "    return retirement_benefit_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1929749b",
   "metadata": {},
   "source": [
    "### 1.3 generating summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adf512be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Define the Agent State ---\n",
    "# This dictionary will hold the state of our application as it runs.\n",
    "class AgentState(TypedDict):\n",
    "    topic: str\n",
    "    pdf_paths: List[str]\n",
    "    documents: List[str]\n",
    "    summary: str\n",
    "\n",
    "# --- 3. Define the Nodes of the Graph ---\n",
    "def load_and_split_pdfs(state: AgentState):\n",
    "    \"\"\"\n",
    "    Loads the PDF files and splits them into chunks.\n",
    "    \"\"\"\n",
    "    topic = state.get(\"topic\")\n",
    "    pdf_paths = state.get(\"pdf_paths\")\n",
    "    \n",
    "    all_docs = []\n",
    "    for pdf_path in pdf_paths:\n",
    "        loader = PyPDFLoader(pdf_path)\n",
    "        docs = loader.load()\n",
    "        all_docs.extend(docs)\n",
    "\n",
    "    #print(all_docs)\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    split_documents = text_splitter.split_documents(all_docs)\n",
    "    \n",
    "    # Convert Document objects to strings for easier processing\n",
    "    documents_as_strings = [doc.page_content for doc in split_documents]\n",
    "\n",
    "    return {\"documents\": documents_as_strings, \"topic\": topic}\n",
    "\n",
    "def summarize(state: AgentState):\n",
    "    \"\"\"\n",
    "    Summarizes the relevant documents based on the topic.\n",
    "    This node acts as our summarization agent.\n",
    "    \"\"\"\n",
    "    \n",
    "    topic = state.get(\"topic\")\n",
    "    documents = state.get(\"documents\")\n",
    "\n",
    "    # Create a retriever to find the most relevant document chunks\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    vectorstore = FAISS.from_texts(documents, embedding=embeddings)\n",
    "    retriever = vectorstore.as_retriever()\n",
    "    \n",
    "    relevant_docs = retriever.invoke(topic)\n",
    "    \n",
    "    # Convert relevant Document objects to a single string\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in relevant_docs])\n",
    "\n",
    "    # Define the prompt for our summarization agent\n",
    "    prompt_template = \"\"\"You are an expert summarizer. Your task is to provide a concise 150 words summary of the following text, focusing on the key points related to the topic: {topic}.\n",
    "\n",
    "    Text to summarize:\n",
    "    {context}\n",
    "\n",
    "    Concise Summary:\"\"\"\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_template(prompt_template)\n",
    "    \n",
    "    llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "    \n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    \n",
    "    summary = chain.invoke({\"context\": context, \"topic\": topic})\n",
    "    \n",
    "    #print(\"---SUMMARY GENERATED---\")\n",
    "    #print(summary)\n",
    "\n",
    "    return {\"summary\": summary}\n",
    "\n",
    "\n",
    "\n",
    "# --- 4. Build the Graph ---\n",
    "\n",
    "# Initialize the graph\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Add the nodes\n",
    "workflow.add_node(\"load_and_split\", load_and_split_pdfs)\n",
    "workflow.add_node(\"summarize\", summarize)\n",
    "\n",
    "# Set the entry point\n",
    "workflow.set_entry_point(\"load_and_split\")\n",
    "\n",
    "# Add the edges\n",
    "workflow.add_edge(\"load_and_split\", \"summarize\")\n",
    "workflow.add_edge(\"summarize\", END)\n",
    "\n",
    "# Compile the graph\n",
    "app = workflow.compile()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6ab104",
   "metadata": {},
   "source": [
    "### 1.4 run the application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0cda771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---SAVED EXTRACTED DATA TO extracted_data.json FILE in task_1_output folder---\n",
      "\n",
      "---GENERATED SUMMARY---\n",
      "The portfolio's performance from January 1, 2021, to March 31, 2021, shows a balance increase from $216,284.58 to $228,743.55, with gains of $12,458.97. The plan includes annuity contracts with TIAA and CREF. A delayed vesting provision may apply, affecting the vested status of employer contributions and earnings based on the participant's length of employment. However, specific vested percentages and market values are not displayed, as the employer maintains this information. Participants are advised to refer to the Summary Plan Description for detailed vesting rules. The plan year, a 12-month period, is used for calculating vesting and distribution. Participants should be informed of any material changes through a revised Summary Plan Description or a Summary of Material Modifications. The trustee, under the direction of a named fiduciary, can appoint investment managers for the plan's assets. Participants are encouraged to sign up for eDelivery to protect personal information.\n"
     ]
    }
   ],
   "source": [
    "# Some variables to hold the path to the PDF file\n",
    "pdf_paths = [\"Data/PDF_Participant-statement-sample2.pdf\", \"Data/your-retirement-benefits.pdf\"]\n",
    "\n",
    "\n",
    "# For Participant Statement\n",
    "full_text_participant = pdf_reading(pdf_paths[0])\n",
    "extracted_participant_data = extract_participant_statement_data(full_text_participant)\n",
    "\n",
    "# For Retirement Benefits\n",
    "full_text_retirement = pdf_reading(pdf_paths[1])\n",
    "extracted_retirement_data = extract_retirement_benefit_data(full_text_retirement)\n",
    "\n",
    "# Combine the extracted data\n",
    "combined_data = {**extracted_participant_data, **extracted_retirement_data}\n",
    "\n",
    "# Save to JSON\n",
    "with open(\"task_1_output/extracted_data.json\", \"w\") as f:\n",
    "    json.dump(combined_data, f, indent=4)\n",
    "print(\"\\n---SAVED EXTRACTED DATA TO extracted_data.json FILE in task_1_output folder---\")\n",
    "\n",
    "\n",
    "# Generate a summary report\n",
    "\n",
    "# Define the inputs for the graph\n",
    "inputs = {\n",
    "    \"topic\": \"The portfolio's performance and allocation. How the plan's vesting rules apply to the participant's scenario\",\n",
    "    \"pdf_paths\": [\"Data/PDF_Participant-statement-sample2.pdf\", \"Data/your-retirement-benefits.pdf\"]\n",
    "}\n",
    "\n",
    "# Run the graph and get the final state\n",
    "final_state = app.invoke(inputs)\n",
    "\n",
    "print(\"\\n---GENERATED SUMMARY---\")\n",
    "print(final_state['summary'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
